# Set environment variables in the format KEY=VALUE, 1 per line
# This file will be sourced inside the project container when started.
# NOTE: If you change this file while the project is running, you must restart the project container for changes to take effect.

MILVUS_VERSION=v2.4.0
APP_MILVUS__URL=http://milvus:19530
APP_REDIS_DSN=redis://redis:6379/0
NGC_CLI_ORG=default
NGC_HOME=~/.cache/nvidia-nims
NGC_API_KEY=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

LLM_NIM_0_MODEL=meta/llama3-8b-instruct
LLM_NIM_0_NIM_VERSION=1.0.0
LLM_NIM_0_NIM_GPUS=all
LLM_NIM_1_MODEL=meta/llama3-8b-instruct
LLM_NIM_1_NIM_VERSION=1.0.0
LLM_NIM_1_NIM_GPUS=all

NV_RERANKQA_MISTRAL_4B_V3_MODEL=nvidia/nv-rerankqa-mistral-4b-v3
NV_RERANKQA_MISTRAL_4B_V3_NIM_VERSION=1.0.2
NV_RERANKQA_MISTRAL_4B_V3_NIM_GPUS=all

NV_EMBEDQA_E5_V5_MODEL=nvidia/nv-embedqa-e5-v5
NV_EMBEDQA_E5_V5_NIM_VERSION=1.0.1
NV_EMBEDQA_E5_V5_NIM_GPUS=all

SHELL=/bin/bash

# Hardcode these to ensure the Python app ignores the cloud defaults
NVIDIA_LLM_URL=http://host.docker.internal:8000/v1
NVIDIA_EMBED_URL=http://host.docker.internal:8001/v1
LLM_MODEL_NAME=meta/llama-3.1-8b-instruct
EMBEDDING_MODEL_NAME=nvidia/llama-3.2-nemoretriever-300m-embed-v1

